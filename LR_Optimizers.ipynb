{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiaaEssam/Convolution_model_Step_by_Step/blob/main/LR_Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyPqZRJO-Oi2"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tifpZDxG0U7T",
        "outputId": "36f97bc4-4aa9-44ed-8354-3b4838a0a40c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'over': 'ignore', 'under': 'ignore', 'invalid': 'warn'}"
            ]
          },
          "metadata": {},
          "execution_count": 388
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "tf.test.gpu_device_name()\n",
        "np.seterr(over='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOIsaqeP-VvG"
      },
      "source": [
        "# Loading MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGd-IRay-Ii7"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_val, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-cq7oJ7-bsT"
      },
      "source": [
        "# Subsetting The Data to The Classes 0 and 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5pwtC0HbTgy"
      },
      "outputs": [],
      "source": [
        "# Storing indeces of class 0 and class 1\n",
        "idx_train_0 = np.where(y_train == 0)[0]\n",
        "idx_train_1 = np.where(y_train == 1)[0]\n",
        "idX_val_0 = np.where(y_test == 0)[0]\n",
        "idX_val_1 = np.where(y_test == 1)[0]\n",
        "\n",
        "# Concatenating the classes for train and test for each variable\n",
        "X_train = np.concatenate((X_train[idx_train_0], X_train[idx_train_1]), axis=0)\n",
        "y_train = np.concatenate((y_train[idx_train_0], y_train[idx_train_1]), axis=0)\n",
        "X_val = np.concatenate((X_val[idX_val_0], X_val[idX_val_1]), axis=0)\n",
        "y_test = np.concatenate((y_test[idX_val_0], y_test[idX_val_1]), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLhxW1OG_LBC"
      },
      "source": [
        "# Visualizing some of The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "t4asSvtF-TKt",
        "outputId": "0ef96bd1-22b6-4d5a-cda2-24d007317efd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for i in range(9):  \n",
        "  plt.subplot(330 + 1 + i)\n",
        "  plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAsr_mZx-bfr"
      },
      "source": [
        "# Transforming The Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSs-MKw_-c5F",
        "outputId": "d3e4c763-2fe4-427f-a9eb-8d06b179f9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "\n",
        "X_train=X_train.reshape(X_train.shape[0],-1)\n",
        "X_val=X_val.reshape(X_val.shape[0],-1)\n",
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-AesVKU-Ms6",
        "outputId": "d88c4e0f-04d3-4c7d-b5be-1bc7f0aae80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (12665, 784)\n",
            "Y_train: (12665,)\n",
            "X_val:  (2115, 784)\n",
            "Y_test:  (2115,)\n"
          ]
        }
      ],
      "source": [
        "print('X_train: ' + str(X_train.shape))\n",
        "print('Y_train: ' + str(y_train.shape))\n",
        "print('X_val:  '  + str(X_val.shape))\n",
        "print('Y_test:  '  + str(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vchqJ-h5_VLX"
      },
      "source": [
        "# Standardizing The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34rMdf5Z8zHI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Standardizing both the training set and test set using the mean and standard deviation of the training set,\n",
        "This is because it ensures that the test set is processed in the same way as the training set,\n",
        "and that the model's performance is evaluated on data that it has not seen before, without any information from the test set leaking into the training set.\n",
        "see the following link: https://stats.stackexchange.com/questions/202287/why-standardization-of-the-testing-set-has-to-be-performed-with-the-mean-and-sd\n",
        "\"\"\"\n",
        "def standardize(X,X_train):\n",
        "  return (X-np.mean(X_train,axis=0))/(np.std(X_train,axis=0) + 10e-16) # we need epsilon for feature (pixel) that has zero variance\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=standardize(X_train,X_train)\n",
        "X_val=standardize(X_val,X_val)\n",
        "#X_val=standardize(X_val,X_train) # correct way"
      ],
      "metadata": {
        "id": "Q9JTBiyqyoRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXDQQG1__mxa"
      },
      "source": [
        "# Shuffle Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRaAlafLpTtV"
      },
      "outputs": [],
      "source": [
        "def Shuffle(X,y):\n",
        "  # Storing indeces of data\n",
        "  indices = np.arange(X.shape[0])\n",
        "  # Shuffling the indeces\n",
        "  np.random.shuffle(indices)\n",
        "  # update the data and target with the new shuffled indeces\n",
        "  X = X[indices]\n",
        "  y = y[indices]\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMY6fej8AZc9"
      },
      "source": [
        "# Accuracy Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lvpVJHlAex9"
      },
      "outputs": [],
      "source": [
        "def Accuracy(y_pred,y_test):\n",
        "\n",
        "    Acc=(np.sum(y_pred==y_test)/len(y_test)) * 100\n",
        "    return round(Acc,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "basOFCu1BzrN"
      },
      "source": [
        "# Error Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1BMRDhxB3Gx"
      },
      "outputs": [],
      "source": [
        "def Error(y_pred,y_test,W,Lambda,reg):\n",
        "  Cost = (np.sum((y_test * np.log(y_pred+1e-10) + (1-y_test) * np.log(1-y_pred+1e-10))) * -1/len(y_test))\n",
        "  return  Cost + ((Lambda * np.sum(np.abs(W)))/ (2 * len(y_test))) if reg else Cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9TMd5LyEkwd"
      },
      "source": [
        "# Sigmoid Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGDhlTs8Enpr"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "DMFF0mtdgMGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(W,b,X):\n",
        "  return np.dot(W,X.T) + b"
      ],
      "metadata": {
        "id": "WDyTU1vjgQOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Net input and Error"
      ],
      "metadata": {
        "id": "MtUAwaJyoiGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Net_and_Error(W,b,X,y,Lambda,reg):\n",
        "  yhat= model(W,b,X)\n",
        "  Z=sigmoid(yhat)\n",
        "  error= Error(Z,y,W,Lambda,reg)\n",
        "  return Z,error"
      ],
      "metadata": {
        "id": "RBWmVqqBon-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient"
      ],
      "metadata": {
        "id": "CRMirFJRgnqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Gradient(Z,y,X):\n",
        "  return np.mean((Z-y) * X.T),np.mean(Z-y)"
      ],
      "metadata": {
        "id": "-ovPTFyhgwkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get W_b Function"
      ],
      "metadata": {
        "id": "7jQHPkRd0bmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_method(X,y,W,b,error,Lambda,eta,reg):\n",
        "        Z,error=Net_and_Error(W,b,X,y,Lambda,reg)\n",
        "\n",
        "        \"\"\"\n",
        "        as the derivative of |X| is |X|/X is not defined at X=0 then we could use a trick which is the sign() function in numpy,\n",
        "        and this trick is called the subdifferential \n",
        "        sign(x) = 1 if x > 0\n",
        "                = 0 if x = 0\n",
        "                = -1 if x < 0\n",
        "        \"\"\"\n",
        "        dw,db=Gradient(Z,y,X)\n",
        "        W= W - eta * (dw) + ((Lambda / ( 2 * len(y) )) * np.sign(W)) if reg else W -( eta * dw)\n",
        "        b= b- eta * db\n",
        "        return W,b,error"
      ],
      "metadata": {
        "id": "OAvuqf9M0nU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing RMS parameters"
      ],
      "metadata": {
        "id": "qKhHDa0rjXjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Initialize_RMS(beta,epsilon,Vdw,vdb):\n",
        "  return beta,epsilon,Vdw,vdb"
      ],
      "metadata": {
        "id": "vpXsrTcSjcua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initializing Adam parameters"
      ],
      "metadata": {
        "id": "UdysXYG8NnHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Initialize_Adam(beta1,beta2,epsilon,Vdw,Sdw,Vdb,Sdb):\n",
        "  return beta1,beta2,epsilon,Vdw,Sdw,Vdb,Sdb"
      ],
      "metadata": {
        "id": "Apacv4mLNtJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMS Function"
      ],
      "metadata": {
        "id": "ovuMF5etexNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RMS(beta,epsilon,t,W,b,dw,db,VdW,Vdb,eta):\n",
        "  \"\"\"\n",
        "  -The RMSProp (Root Mean Square Propagation)\n",
        "  -The RMSProp optimizer works by maintaining a moving average of the squared gradient for each weight\n",
        "  -The RMSProp optimizer uses a decay rate/beta hyperparameter to control the weighting of past and current gradients.\n",
        "  -This decay rate determines how much weight is given to the most recent gradient compared to the historical gradients.\n",
        "  -A larger decay rate means that the moving average will be more heavily influenced by the historical gradients,\n",
        "  -while a smaller decay rate means that the moving average will be more influenced by the most recent gradient.  \n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  -Vdw is the moving average of the squared gradients of the weights, and Vdb is the moving average of the squared gradients of the biases. \n",
        "  -These moving averages are used to adjust the learning rate during training, with the goal of improving the convergence of the optimization algorithm.\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  -The letter \"V\" in Vdw and Vdb stands for \"variance\". In the context of the RMSProp optimizer, Vdw and Vdb represent the variance of the gradients of the weights and biases, respectively.\n",
        "  -The use of variance in the RMSProp optimizer is based on the intuition that the variance of the gradients can provide information about the scale of the gradients.\n",
        "  -If the variance of the gradients is high, it suggests that the gradients are large and the learning rate should be reduced to prevent overshooting the minimum. Conversely, \n",
        "  -if the variance of the gradients is low, it suggests that the gradients are small and the learning rate can be increased to speed up convergence.\n",
        "  \"\"\"\n",
        "  VdW = ((beta * VdW) + ((1 - beta) * (dw**2))) / (1 - beta**t + epsilon)\n",
        "  Vdb = ((beta * Vdb) + ((1 - beta) * (db**2))) / (1 - beta**t + epsilon)\n",
        "  W = W - (eta * (dw/(np.sqrt(VdW) + epsilon)))\n",
        "  b = b - (eta * (db/np.sqrt(Vdb) + epsilon))\n",
        "  return W,b\n"
      ],
      "metadata": {
        "id": "NP8-yozyezq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adam Functon"
      ],
      "metadata": {
        "id": "ZDu8o03K_IIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam(beta1,beta2,t,epsilon,W,b,dw,db,VdW,SdW,Vdb,Sdb,eta):\n",
        "  \"\"\"\n",
        "  -The Adam (Adaptive Moment Estimation) optimizer is a popular optimization algorithm used in deep learning.\n",
        "  -It is an adaptive learning rate optimization algorithm that combines the benefits of two other optimization algorithms, RMSProp and momentum.\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  -The Adam optimizer works by maintaining a moving average of the gradients and the squared gradients of the weights\n",
        "  -VdW = beta1 * VdW + (1 - beta1) * dL/dw\n",
        "  -SdW = beta2 * SdW + (1 - beta2) * (dL/dw)^2\n",
        "  -VdW_corrected = VdW_corrected / (1 - beta1^t)\n",
        "  -SdW_corrected = SdW_corrected / (1 - beta2^t)\n",
        "  -w = w - alpha * VdW_corrected / (sqrt(SdW_corrected) + epsilon)\n",
        "  -the same for b\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  -beta1 and beta2 are hyperparameters\n",
        "  Set β1=0, means that ADAM behaves exactly as RMSprop optimizer\n",
        "  Set β2=0, means that ADAM behaves exactly as Momentum optimizer\n",
        "  \"\"\"\n",
        "  VdW= (beta1 * VdW) + ((1 - beta1) * dw)    # Momentum\n",
        "  Vdb= (beta1 * Vdb) + ((1 - beta1) * db)    # Momentum\n",
        "  SdW= (beta2 * SdW) + ((1 - beta2) * dw**2) # RMS, represents variance that will be square rooted soon \n",
        "  Sdb= (beta2 * Sdb) + ((1 - beta2) * db**2) # RMS, represents variance that will be square rooted soon \n",
        "  \"\"\"\n",
        "  -The bias correction step helps to ensure that the moving averages are unbiased estimates of the true first and second moments of the gradients.\n",
        "  -Without the bias correction step, the moving averages would be biased towards zero, which could lead to slower convergence or suboptimal performance.\n",
        "  \"\"\"\n",
        "  VdW_corrected= VdW/(1 - beta1**t + epsilon) # bias correction\n",
        "  Vdb_corrected= Vdb/(1 - beta1**t + epsilon) # bias correction\n",
        "  SdW_corrected= SdW/(1 - beta2**t + epsilon) # bias correction\n",
        "  Sdb_corrected= Sdb/(1 - beta2**t + epsilon) # bias correction\n",
        "\n",
        "  W = W - eta * (VdW_corrected/np.sqrt(SdW_corrected + epsilon)) # SdW_corrected is standard deviation here\n",
        "  b = b - eta * (Vdb_corrected/np.sqrt(Sdb_corrected + epsilon)) # SdW_corrected is standard deviation here\n",
        "  return W,b"
      ],
      "metadata": {
        "id": "PUmlXhwF_LzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LE8W_94DAit"
      },
      "source": [
        "# Logistic_Regression_Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6-8_lTLDXzD"
      },
      "outputs": [],
      "source": [
        "def Logistic_Regression(X,y,epochs,Lambda,eta,reg,batch,batch_size,rms,adam):\n",
        "    W=np.random.randn(X.shape[1])\n",
        "    b=np.random.randn()\n",
        "    error=0\n",
        "\n",
        "    for j in range(epochs):\n",
        "        num_batches=int(X.shape[0]/batch_size)\n",
        "        X,y=Shuffle(X,y)\n",
        "        for i in range(num_batches):\n",
        "          if batch:  \n",
        "              if i != num_batches-1:        \n",
        "                  W,b,error=batch_method(X[i*batch_size:batch_size*(i+1)],y[i*batch_size:batch_size*(i+1)],W,b,error,Lambda,eta,reg)\n",
        "              else:\n",
        "                  W,b,error=batch_method(X[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],y[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],W,b,error,Lambda,eta,reg)\n",
        "          if rms:\n",
        "              beta,epsilon,Vdw,Vdb = Initialize_RMS(0.9,10e-16,np.zeros(W.shape),0)\n",
        "              Z,error=Net_and_Error(W,b,X[i*batch_size:batch_size*(i+1)],y[i*batch_size:batch_size*(i+1)],Lambda,reg) if i!=num_batches-1 else Net_and_Error(W,b,X[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],y[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],Lambda,reg)\n",
        "              dw,db=Gradient(Z,y[i*batch_size:batch_size*(i+1)],X[i*batch_size:batch_size*(i+1)]) if i!=num_batches-1 else Gradient(Z,y[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],X[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size])\n",
        "              W,b=RMS(beta,epsilon,j,W,b,dw,db,Vdw,Vdb,eta)\n",
        "\n",
        "          if adam:\n",
        "              beta1,beta2,epsilon,VdW,SdW,Vdb,Sdb = Initialize_Adam(0.9,0.999,10e-16,np.zeros(W.shape),np.zeros(W.shape),0,0)\n",
        "              Z,error=Net_and_Error(W,b,X[i*batch_size:batch_size*(i+1)],y[i*batch_size:batch_size*(i+1)],Lambda,reg) if i!=num_batches-1 else Net_and_Error(W,b,X[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],y[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],Lambda,reg)\n",
        "              dw,db=Gradient(Z,y[i*batch_size:batch_size*(i+1)],X[i*batch_size:batch_size*(i+1)]) if i!=num_batches-1 else Gradient(Z,y[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size],X[i*batch_size:batch_size*(i+1) + X.shape[0] % batch_size])\n",
        "              W,b=Adam(beta1,beta2,j,epsilon,W,b,dw,db,VdW,SdW,Vdb,Sdb,eta)\n",
        "        \n",
        "\n",
        "    print(\"Training_Error: \",error)\n",
        "    return W,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI-lyVosGeBJ"
      },
      "source": [
        "# Test_model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcGYqrTRGit5"
      },
      "outputs": [],
      "source": [
        "# function to test on each model\n",
        "def test_model(W,b,X_val,y_test):\n",
        "    y_pred= np.dot(W,X_val.T) + b\n",
        "    Z=sigmoid(y_pred)\n",
        "    Z[Z>0.5]= 1\n",
        "    Z[Z<0.5]= 0\n",
        "    return Accuracy(Z.astype(np.int32),y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU_kFuFBOv9t"
      },
      "outputs": [],
      "source": [
        "def Logistic_Regression_GD(X_train,y_train,X_val,y_test,epochs,Lambda=0.0,eta=0.001,reg=False,batch=False,batch_size=X_train.shape[0],rms=False,adam=False):\n",
        "  W,b=Logistic_Regression(X_train,y_train,epochs,Lambda,eta,reg,batch,batch_size,rms,adam)\n",
        "  Acc=test_model(W,b,X_val,y_test)\n",
        "  return Acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note that we will fix number of epochs to be 25 and learning rate value to be 0.001 for all cases discussed in this notebook, so we can compare fairly as possible as we could."
      ],
      "metadata": {
        "id": "uZhNRQftwEZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Testing GD with L1 Regularization/Lasso"
      ],
      "metadata": {
        "id": "8CeT1S0rsJ3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accs={Lambda:Logistic_Regression_GD(X_train,y_train,X_val,y_test,25,Lambda,0.001,reg=True,batch=True) for Lambda in [0.01,0.1]}\n",
        "print(\"Testing_Accuracies: \",Accs)"
      ],
      "metadata": {
        "id": "Q2Cx_LgI9AZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e761555e-2555-4486-f4ef-eb10bc6d97f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Error:  8.180588840313005\n",
            "Training_Error:  2.5856888324730427\n",
            "Testing_Accuracies:  {0.01: 29.1, 0.1: 78.2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In GD with Lasso:\n",
        "\n",
        "It deals with high-dimensional data, as it can help in reducing the number of features by shrinking the coefficients of less important features to zero.\n",
        "\n",
        "Is we can see, using 0.01 lambda gives us 29.1 accuracy and by increasing lambda to be 0.1 the accuracy goes up more than the one using 0.01 lambda.\n",
        "\n",
        "We could conclude from that the more we increase lambda the more accuracy we get.\n",
        "\n",
        "However using high lambda could affect the model badly as it will neglect many features that may be important thus making the model so simple that may not fit the training data well which makes the model do poorly on the testting data"
      ],
      "metadata": {
        "id": "7kWt4YWCvaKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######For good accuracies try epochs=500,eta=0.1"
      ],
      "metadata": {
        "id": "ZQPRikavuEAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accs={Lambda:Logistic_Regression_GD(X_train,y_train,X_val,y_test,500,Lambda,0.1,reg=True,batch=True) for Lambda in [0.01,0.1]}\n",
        "print(\"Testing_Accuracies: \",Accs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIkCm6mmuI_E",
        "outputId": "bb1af7e4-0b8a-4bec-a1e3-19e7e11c6b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Error:  2.251709707724267\n",
            "Training_Error:  0.9791902190842146\n",
            "Testing_Accuracies:  {0.01: 83.7, 0.1: 91.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Testing Mini Batch GD"
      ],
      "metadata": {
        "id": "dhe4lOA4sefg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accs={batch_size:Logistic_Regression_GD(X_train,y_train,X_val,y_test,25,eta=0.001,batch_size=batch_size,batch=True) for batch_size in [32,64,128,256]} # For good accuracies try epochs=500,eta=0.1\n",
        "print(\"Testing_Accuracies: \",Accs)"
      ],
      "metadata": {
        "id": "O4Ax5GVyshC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8413e234-3fbf-4c26-cbe7-ba0f64961d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Error:  1.77070426260131\n",
            "Training_Error:  2.3767884872176066\n",
            "Training_Error:  2.7881676072267823\n",
            "Training_Error:  1.7562613967234644\n",
            "Testing_Accuracies:  {32: 91.8, 64: 79.3, 128: 75.8, 256: 81.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Mini-Batch GD:\n",
        "\n",
        "It is faster than GD as it updates the model parameters more frequently. It can also help in avoiding local minima by introducing more randomness into the optimization process.\n",
        "\n",
        "It has more variability in steps towards the global minimum than GD which could be the reason why it avoids local minimum that the previous models using GD with Lasso could not avoid thus gave lower accuracies than the accuracies we got using MIni batch GD.\n",
        "\n",
        "As we can see, we used values for batch size such as 32,64,128,256, the reason we used those because they are powers of 2. Using powers of 2 can be beneficial for hardware optimization, as many computer architectures have optimized memory access patterns for powers of 2. This can lead to faster computation times and better memory utilization.\n",
        "\n",
        "Also we can see values of 32 and 256 batch sizes give the highest accuracies.\n"
      ],
      "metadata": {
        "id": "6JNKMl66yWB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######For good accuracies try epochs=500,eta=0.1"
      ],
      "metadata": {
        "id": "sHl0ElFouTWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accs={batch_size:Logistic_Regression_GD(X_train,y_train,X_val,y_test,500,eta=0.1,batch_size=batch_size,batch=True) for batch_size in [32,64,128,256]} # For good accuracies try epochs=500,eta=0.1\n",
        "print(\"Testing_Accuracies: \",Accs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_gJdxYauUYD",
        "outputId": "abf1e657-0a94-4aa5-da24-2d79a8a6e3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Error:  1.479069124339658\n",
            "Training_Error:  1.966076466784516\n",
            "Training_Error:  1.7510401476168862\n",
            "Training_Error:  1.4688943792078875\n",
            "Testing_Accuracies:  {32: 88.8, 64: 86.7, 128: 87.2, 256: 87.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training & Testing Using RMS prop"
      ],
      "metadata": {
        "id": "SLWerwuvUvm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accs=Logistic_Regression_GD(X_train,y_train,X_val,y_test,25,eta=0.001,batch_size=64,rms=True) \n",
        "print(\"Testing_Accuracies: \",Accs)"
      ],
      "metadata": {
        "id": "uotNcHZuValx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ee171d-7cfa-47ce-fb83-97d5cdfb24cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Error:  1.8966207035133407\n",
            "Testing_Accuracies:  85.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In RMSProp:\n",
        "\n",
        "Note that we used Decay Rate/Beta of 0.9.\n",
        "\n",
        "As we can we see, we got higher accuracy using RMSProp.\n",
        "\n",
        "It can help in avoiding the vanishing or exploding gradient problem by adjusting the learning rate based on the magnitude of the gradients.\n",
        "\n",
        "So we could conclude that it makes the steps taken to reach to the global minimum more smoothly than Mini batch GD by adjusting the learing rate depending on 90% of historical gradients and 10% of current gradient which in fact decreases the variability of the movements towards the global minimum thus solving the probelm of high variability in Mini batch GD."
      ],
      "metadata": {
        "id": "GCXkWAMJofma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training & Testing Using Adam"
      ],
      "metadata": {
        "id": "RJwLyCqX3sf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accs=Logistic_Regression_GD(X_train,y_train,X_val,y_test,25,eta=0.001,batch_size=64,adam=True)\n",
        "print(\"Testing_Accuracies: \",Accs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA5U0HXEz1R8",
        "outputId": "f538ced4-c4ec-4062-e1d5-2c067bdfe6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Error:  1.3320740206461432\n",
            "Testing_Accuracies:  93.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Adam:\n",
        "\n",
        "As we can see we got higher accuracy using Adam than Using RMSProp.\n",
        "\n",
        "It uses the mentioned properties of RMSProp,but it adds the idea of momentum.\n",
        "\n",
        "Using momentum allows it to use a running average of the previous updates, it can help in avoiding the problem of getting stuck in local minima and can accelerate the convergence of the optimization process.\n",
        "\n",
        "So it has the the advantage of Mini batch GD and the advantage of RMSProp.\n",
        "\n",
        "Also it applys Bias Correction, This can help in improving the accuracy of the optimization process and can lead to better performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9gsdHj71qMaT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXFDBqpuxnSbUxjGrGhvj3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}